{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 모듈들을 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import save_model,load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input ,GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 배열 생성 및 파일 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O  파일 길이 :  421\n",
      "V  파일 길이 :  465\n",
      "paper  파일 길이 :  435\n",
      "rock  파일 길이 :  450\n",
      "side  파일 길이 :  453\n",
      "ok 2224\n"
     ]
    }
   ],
   "source": [
    "caltech_dir = \"./HandGesture\\images\" #학습데이터\n",
    "categories = [\"O\", \"V\", \"paper\", \"rock\",\"side\"] #카테고리 나누기\n",
    "nb_classes = len(categories)#카테고리 길이 결정\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "pixels = image_h * image_w * 3 #픽셀은 64*64에 3층\n",
    "X = []\n",
    "y = []\n",
    "#카테고리 수만큼 전처리 원핫 인코딩\n",
    "for idx, cat in enumerate(categories): \n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "    image_dir = caltech_dir + \"/\" + cat #cat의 폴더 열기\n",
    "    files = glob.glob(image_dir+\"/*.jpg\") #cat에 있는 jpg로 끝나는 파일 읽어오기\n",
    "    print(cat, \" 파일 길이 : \", len(files))#전체 파일 수 i 파일 수 f는 파일 이름\n",
    "    for i, f in enumerate(files):#파일 수만큼 반복\n",
    "        img = cv2.imread(f)#이미지 열기 cv2로 열기\n",
    "        #손 색상 변경\n",
    "        ycrb = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) #cv2의 색상을 RGB가 아닌 YCrCb로 변경\n",
    "        img = cv2.inRange(ycrb,np.array([0,133,77]),np.array([255,173,127]))  #해당 색(사람 피부색)에 관련된 색을 구분\n",
    "        \n",
    "        img = Image.fromarray(img) #cv2의 이미지를 PIL형식의 이미지로 변경\n",
    "        img = img.convert(\"RGB\")#흑백\n",
    "        img = img.resize((image_w, image_h))#사이즈 변경\n",
    "        # img.show()\n",
    "        data = np.asarray(img)#np.array로 변경\n",
    "        X.append(data) #x에 데이터 \n",
    "        y.append(label) #y에 출력 설정\n",
    "\n",
    "X = np.array(X) # 데이터 전부 모아서 배열 성정\n",
    "y = np.array(y)\n",
    "\n",
    "#학습집합 나누어 주기\n",
    "\n",
    "#X_train y_train 최종학습 집합\n",
    "#X_train_f y_train_f 학습 집합\n",
    "#x_test y_test 테스트 집합\n",
    "#X_vailed y_vailed 검증 집합\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "X_train_f, X_vailed, y_train_f, y_vailed = train_test_split(X_train,y_train)\n",
    "xy = (X_train, X_test, y_train, y_test,X_train_f, X_vailed, y_train_f, y_vailed)\n",
    "\n",
    "\n",
    "np.savez(\"./HandGesture/multi_image_data.npy\", X_train=X_train, X_test=X_test,y_train=y_train,y_test=y_test,X_train_f=X_train_f,X_vailed=X_vailed, y_train_f=y_train_f,y_vailed=y_vailed )\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장했던 데이터 불러오기 및 이미지 증식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증식 옵션\n",
    "train_datagen = ImageDataGenerator(rescale = 1/255,rotation_range=30,width_shift_range=0.1,height_shift_range=0.1,shear_range=0.5,zoom_range=0.3,horizontal_flip=True,vertical_flip=True,validation_split=0.2)\n",
    "\n",
    "# 저장된 데이터 불러오기\n",
    "# X_train, X_test, y_train, y_test,X_train_f, X_vailed, y_train_f, y_vailed = np.load('./HandGesture/multi_image_data.npy',allow_pickle=True)\n",
    "\n",
    "xy = np.load('./HandGesture/multi_image_data.npz')\n",
    "X_train = xy['X_train']\n",
    "X_test = xy['X_test']\n",
    "y_train = xy['y_train']\n",
    "y_test = xy['y_test']\n",
    "X_train_f = xy['X_train_f']\n",
    "X_vailed = xy['X_vailed']\n",
    "y_train_f = xy['y_train_f']\n",
    "y_vailed = xy['y_vailed']\n",
    "\n",
    "\n",
    "#학습집합의 데이터 증식\n",
    "train_generator = train_datagen.flow(\n",
    "    x=X_train,y=y_train,\n",
    "    batch_size = 32,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "#최종 학습 집합 데이터 증식\n",
    "train_f_generator = train_datagen.flow(\n",
    "    x=X_train_f,y=y_train_f,\n",
    "    batch_size = 32,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "#검증 집합 데이터 증식\n",
    "vailed_generator = train_datagen.flow(\n",
    "    x=X_vailed,y=y_vailed,\n",
    "    batch_size = 32,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "#테스트 집합 데이터 증식\n",
    "test_generator = train_datagen.flow(\n",
    "    x=X_test,y=y_test,\n",
    "    batch_size = 32,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#카테고리 설정\n",
    "categories = [\"O\", \"V\", \"paper\", \"rock\",\"side\"] \n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255\n",
    "X_train_f = X_train_f.astype(float) /255\n",
    "X_vailed = X_vailed.astype(float) /255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그냥 인터넷에서 가져온 신경망 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Local\\Temp\\ipykernel_12312\\4009080640.py:27: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history_1 = model.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 19s 408ms/step - loss: 1.7178 - accuracy: 0.2414 - val_loss: 1.5878 - val_accuracy: 0.3429\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 15s 374ms/step - loss: 1.5520 - accuracy: 0.2958 - val_loss: 1.4531 - val_accuracy: 0.3981\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 15s 370ms/step - loss: 1.4662 - accuracy: 0.3573 - val_loss: 1.2554 - val_accuracy: 0.5396\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 15s 376ms/step - loss: 1.3446 - accuracy: 0.4420 - val_loss: 1.1699 - val_accuracy: 0.5492\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 1.2687 - accuracy: 0.4724 - val_loss: 1.1919 - val_accuracy: 0.5923\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 15s 381ms/step - loss: 1.1690 - accuracy: 0.5516 - val_loss: 1.1714 - val_accuracy: 0.5875\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 15s 376ms/step - loss: 1.0605 - accuracy: 0.5963 - val_loss: 0.8239 - val_accuracy: 0.6835\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 15s 377ms/step - loss: 0.8715 - accuracy: 0.6627 - val_loss: 0.6313 - val_accuracy: 0.7722\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 16s 407ms/step - loss: 0.8195 - accuracy: 0.6875 - val_loss: 0.6292 - val_accuracy: 0.7914\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 15s 385ms/step - loss: 0.7406 - accuracy: 0.7338 - val_loss: 0.6094 - val_accuracy: 0.7746\n",
      "INFO:tensorflow:Assets written to: ./model/model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', kernel_initializer='he_uniform', padding='same')\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[64, 64, 3]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(), # 2차원을 1차원으로 변환하여 밀집층으로 연결\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(nb_classes, activation='softmax'),\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_dir = './model'\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model1.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_1 = model.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "model.save(\"./model/model_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 합성곱 + 최대풀링 섞은 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Local\\Temp\\ipykernel_12312\\1441680498.py:23: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history_2 = model2.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 8s 172ms/step - loss: 1.4720 - accuracy: 0.4468 - val_loss: 1.1034 - val_accuracy: 0.6043\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 7s 174ms/step - loss: 1.0462 - accuracy: 0.6035 - val_loss: 1.0371 - val_accuracy: 0.6043\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.8901 - accuracy: 0.6579 - val_loss: 0.7148 - val_accuracy: 0.7770\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 0.7820 - accuracy: 0.7570 - val_loss: 0.7066 - val_accuracy: 0.7818\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 6s 159ms/step - loss: 0.6331 - accuracy: 0.7794 - val_loss: 0.5836 - val_accuracy: 0.8010\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 6s 160ms/step - loss: 0.4911 - accuracy: 0.8345 - val_loss: 0.4525 - val_accuracy: 0.8657\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 0.4344 - accuracy: 0.8609 - val_loss: 0.4425 - val_accuracy: 0.8657\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 6s 160ms/step - loss: 0.4818 - accuracy: 0.8441 - val_loss: 0.5084 - val_accuracy: 0.8345\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 6s 160ms/step - loss: 0.4366 - accuracy: 0.8417 - val_loss: 0.4423 - val_accuracy: 0.8729\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 6s 159ms/step - loss: 0.4702 - accuracy: 0.8529 - val_loss: 0.3861 - val_accuracy: 0.8753\n",
      "INFO:tensorflow:Assets written to: ./model/model_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/model_2\\assets\n"
     ]
    }
   ],
   "source": [
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', kernel_initializer='he_uniform', padding='same')\n",
    "\n",
    "model2 = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[64, 64, 3]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(nb_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_dir = './model'\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model2.h5\", save_best_only=True)\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_2 = model2.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "model2.save(\"./model/model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 모델 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Local\\Temp\\ipykernel_12312\\179852561.py:29: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history_3 = model3.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 300ms/step - loss: 1.0228 - acc: 0.5939 - val_loss: 0.4512 - val_acc: 0.8561\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.5435 - acc: 0.8129 - val_loss: 0.3015 - val_acc: 0.8825\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4437 - acc: 0.8385 - val_loss: 0.2956 - val_acc: 0.8753\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.3822 - acc: 0.8721 - val_loss: 0.2244 - val_acc: 0.9161\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.3647 - acc: 0.8793 - val_loss: 0.2065 - val_acc: 0.9257\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.3151 - acc: 0.8841 - val_loss: 0.1566 - val_acc: 0.9424\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 0.2838 - acc: 0.9017 - val_loss: 0.1639 - val_acc: 0.9424\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 0.2722 - acc: 0.9089 - val_loss: 0.1987 - val_acc: 0.9400\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.2182 - acc: 0.9265 - val_loss: 0.2046 - val_acc: 0.9281\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.2279 - acc: 0.9233 - val_loss: 0.1667 - val_acc: 0.9400\n",
      "INFO:tensorflow:Assets written to: ./model/model_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/model_3\\assets\n"
     ]
    }
   ],
   "source": [
    "def model_maker():\n",
    "    base_model = VGG16(include_top=False, input_shape=(64, 64, 3)) #베이스 모델로는 VGG16 모델을\n",
    "\n",
    "    for layer in base_model.layers[:-2]:\n",
    "        layer.trainable = False # Top 층을 제외한 나머지 층에서 2개의 층을 새롭게 학습 \n",
    " \n",
    "    input1 = Input(shape=(64, 64, 3))\n",
    "    custom_model = base_model(input1)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(nb_classes, activation='softmax')(custom_model)\n",
    "    return Model(inputs=input1, outputs=predictions)\n",
    "model3 = model_maker()\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_dir = './model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = 'model3.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "\n",
    "history_3 = model3.fit_generator(train_f_generator, epochs=10, validation_data=vailed_generator,\n",
    "                   callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model3.save(\"./model/model_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 74ms/step - loss: 0.4778 - accuracy: 0.8417\n",
      "모델1 정확도 : 0.8417\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.2582 - accuracy: 0.9245\n",
      "모델2(합성곱+최대풀링) 정확도 : 0.9245\n",
      "18/18 [==============================] - 3s 178ms/step - loss: 0.1473 - acc: 0.9388\n",
      "모델3(VGG모델) 정확도 : 0.9388\n"
     ]
    }
   ],
   "source": [
    "print(\"모델1 정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))\n",
    "print(\"모델2(합성곱+최대풀링) 정확도 : %.4f\" % (model2.evaluate(X_test, y_test)[1]))\n",
    "print(\"모델3(VGG모델) 정확도 : %.4f\" % (model3.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg로 최종 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4= model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Local\\Temp\\ipykernel_12312\\2840650790.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history_4 = model4.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 287ms/step - loss: 0.2325 - acc: 0.9221 - val_loss: 0.1611 - val_acc: 0.9335\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 15s 286ms/step - loss: 0.1849 - acc: 0.9269 - val_loss: 0.1579 - val_acc: 0.9406\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 15s 286ms/step - loss: 0.1938 - acc: 0.9353 - val_loss: 0.1691 - val_acc: 0.9442\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 15s 287ms/step - loss: 0.1782 - acc: 0.9382 - val_loss: 0.1530 - val_acc: 0.9496\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 15s 292ms/step - loss: 0.1805 - acc: 0.9347 - val_loss: 0.1621 - val_acc: 0.9388\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 15s 293ms/step - loss: 0.1535 - acc: 0.9454 - val_loss: 0.1373 - val_acc: 0.9478\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 15s 286ms/step - loss: 0.1580 - acc: 0.9460 - val_loss: 0.1558 - val_acc: 0.9424\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 15s 290ms/step - loss: 0.1735 - acc: 0.9430 - val_loss: 0.1566 - val_acc: 0.9478\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 0.1306 - acc: 0.9502 - val_loss: 0.1617 - val_acc: 0.9478\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 0.1475 - acc: 0.9562 - val_loss: 0.1574 - val_acc: 0.9478\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 15s 285ms/step - loss: 0.1339 - acc: 0.9514 - val_loss: 0.1471 - val_acc: 0.9478\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 16s 295ms/step - loss: 0.1345 - acc: 0.9496 - val_loss: 0.1536 - val_acc: 0.9478\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 15s 292ms/step - loss: 0.1082 - acc: 0.9604 - val_loss: 0.1212 - val_acc: 0.9550\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 16s 293ms/step - loss: 0.1152 - acc: 0.9622 - val_loss: 0.1421 - val_acc: 0.9478\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 16s 293ms/step - loss: 0.1093 - acc: 0.9610 - val_loss: 0.1176 - val_acc: 0.9568\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.0980 - acc: 0.9622 - val_loss: 0.1422 - val_acc: 0.9442\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 15s 290ms/step - loss: 0.1253 - acc: 0.9598 - val_loss: 0.1393 - val_acc: 0.9514\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.1241 - acc: 0.9526 - val_loss: 0.1337 - val_acc: 0.9532\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 15s 292ms/step - loss: 0.0762 - acc: 0.9706 - val_loss: 0.1058 - val_acc: 0.9568\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 15s 290ms/step - loss: 0.1403 - acc: 0.9586 - val_loss: 0.1256 - val_acc: 0.9514\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 16s 300ms/step - loss: 0.1042 - acc: 0.9622 - val_loss: 0.1275 - val_acc: 0.9496\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 15s 292ms/step - loss: 0.1331 - acc: 0.9556 - val_loss: 0.1177 - val_acc: 0.9496\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.1097 - acc: 0.9604 - val_loss: 0.1062 - val_acc: 0.9604\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.1014 - acc: 0.9658 - val_loss: 0.1089 - val_acc: 0.9604\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 15s 292ms/step - loss: 0.0889 - acc: 0.9682 - val_loss: 0.1952 - val_acc: 0.9335\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 15s 295ms/step - loss: 0.1025 - acc: 0.9676 - val_loss: 0.1087 - val_acc: 0.9640\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 16s 297ms/step - loss: 0.0717 - acc: 0.9724 - val_loss: 0.0779 - val_acc: 0.9658\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.0839 - acc: 0.9682 - val_loss: 0.1260 - val_acc: 0.9514\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 15s 292ms/step - loss: 0.0889 - acc: 0.9706 - val_loss: 0.1191 - val_acc: 0.9496\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 15s 291ms/step - loss: 0.0997 - acc: 0.9652 - val_loss: 0.1232 - val_acc: 0.9640\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 16s 305ms/step - loss: 0.0909 - acc: 0.9676 - val_loss: 0.0986 - val_acc: 0.9676\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 16s 304ms/step - loss: 0.0699 - acc: 0.9736 - val_loss: 0.1036 - val_acc: 0.9622\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 16s 303ms/step - loss: 0.0959 - acc: 0.9682 - val_loss: 0.0932 - val_acc: 0.9568\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 16s 305ms/step - loss: 0.1067 - acc: 0.9676 - val_loss: 0.1159 - val_acc: 0.9676\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 16s 303ms/step - loss: 0.0993 - acc: 0.9682 - val_loss: 0.1048 - val_acc: 0.9658\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 16s 300ms/step - loss: 0.0882 - acc: 0.9706 - val_loss: 0.0989 - val_acc: 0.9604\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 16s 303ms/step - loss: 0.0830 - acc: 0.9748 - val_loss: 0.1266 - val_acc: 0.9532\n",
      "INFO:tensorflow:Assets written to: ./model/model_Last\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/model_Last\\assets\n"
     ]
    }
   ],
   "source": [
    "#최종모델 불러오기\n",
    "# model4 = load_model('./model/model_Last')\n",
    "\n",
    "history_4 = model4.fit_generator(train_generator,\n",
    "                        epochs=100,\n",
    "                        validation_data=test_generator,\n",
    "                        callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "model4.save(\"./model/model_Last\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 모델 성능 (테스트 집합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 3s 178ms/step - loss: 0.0678 - acc: 0.9766\n",
      "최종 모델 정확도 : 0.9766\n"
     ]
    }
   ],
   "source": [
    "print(\"최종 모델 정확도 : %.4f\" % (model4.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반 사진으로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 298ms/step\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_213403_001.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_213403_002.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_213403_003.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_213403_004.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_213403_005.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 20221127_214509_001.jpg이미지는 V으로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 20221127_214509_002.jpg이미지는 V으로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 20221127_214509_003.jpg이미지는 V으로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 20221127_214509_004.jpg이미지는 V으로 추정됩니다.\n",
      "[0.000 1.000 0.000 0.000 0.000]\n",
      "1\n",
      "해당 20221127_214509_005.jpg이미지는 V으로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_214801_084.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_214812_040.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_214812_048.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_214819_004.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221127_214819_068.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221201_212243_001.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221201_212243_002.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221201_212243_003.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221201_212243_004.jpg이미지는 paper로 추정됩니다.\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "해당 20221201_212243_005.jpg이미지는 paper로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 20230904_014415_094.jpg이미지는 o로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 20230904_014415_095.jpg이미지는 o로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 20230904_014415_096.jpg이미지는 o로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 WIN_20230904_01_48_42_Pro.jpg이미지는 o로 추정됩니다.\n",
      "[1.000 0.000 0.000 0.000 0.000]\n",
      "0\n",
      "해당 WIN_20230904_01_48_44_Pro.jpg이미지는 o로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "caltech_dir = \"./HandGesture/test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    img = cv2.imread(f)#이미지 열고\n",
    "    #손 색상 변경\n",
    "    ycrb = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "    img = cv2.inRange(ycrb,np.array([0,133,77]),np.array([255,173,127])) \n",
    "    img = Image.fromarray(img)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    #img.show()\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "model = load_model('./model/model_Last')\n",
    "\n",
    "prediction = model.predict(X)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"o\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"V\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"paper\"\n",
    "    elif pre_ans == 3: pre_ans_str = \"rock\"\n",
    "    else: pre_ans_str = \"side\"\n",
    "   \n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 화면으로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:screen_brightness_control.windows:exception parsing edid str for DISPLAY\\AUO2992\\5&6079e44&0&UID512_0 - EDIDParseError: parsed EDID returned invalid display name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "paper\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[0.000 0.000 1.000 0.000 0.000]\n",
      "2\n",
      "paper\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\vscode\\python\\GestureControl\\gesture.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode/python/GestureControl/gesture.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m y\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode/python/GestureControl/gesture.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/vscode/python/GestureControl/gesture.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m capture\u001b[39m.\u001b[39;49mread() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode/python/GestureControl/gesture.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame,cv2\u001b[39m.\u001b[39mCOLOR_BGR2YCrCb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode/python/GestureControl/gesture.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m, frame)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tkinter import * #GUI\n",
    "import numpy as np \n",
    "import cv2 #웹캠\n",
    "import screen_brightness_control as sbc #화면 밝기 조절\n",
    "import pyautogui #볼륨 업 다운\n",
    "import keyboard\n",
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "def chgkey(x):\n",
    "    if x == 1: #o\n",
    "        pyautogui.press('volumeup')\n",
    "    elif x == 2: #v\n",
    "        pyautogui.press('volumedown')\n",
    "    elif x == 3: #r\n",
    "        pyautogui.press('volumemute')   \n",
    "\n",
    "def cam():\n",
    "    cv2.imwrite(\"self camera test.jpg\", frame) # 사진 저장\n",
    "\n",
    "\n",
    "brightness = sbc.get_brightness()\n",
    "b = brightness[0]\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "y=1\n",
    "\n",
    "while(True):\n",
    "    ret, frame = capture.read() \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2YCrCb)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cam()\n",
    "        \n",
    "        caltech_dir = \"./\"\n",
    "        image_w = 64\n",
    "        image_h = 64\n",
    "\n",
    "        X = []\n",
    "        filenames = []\n",
    "        files = glob.glob(caltech_dir+\"self camera test.jpg\")\n",
    "\n",
    "        for i, f in enumerate(files):\n",
    "            img = cv2.imread(f)#이미지 열고\n",
    "            #손 색상 변경\n",
    "            ycrb = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "            img = cv2.inRange(ycrb,np.array([0,133,77]),np.array([255,173,127])) \n",
    "            img = Image.fromarray(img)\n",
    "            img = img.convert(\"RGB\")\n",
    "            img = img.resize((image_w, image_h))\n",
    "            #img.show()\n",
    "            data = np.asarray(img)\n",
    "            filenames.append(f)\n",
    "            X.append(data)\n",
    "\n",
    "        X = np.array(X)\n",
    "        model = load_model('./model/model_Last')\n",
    "\n",
    "        prediction = model.predict(X)\n",
    "        np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "        cnt = 0\n",
    "        #이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "        for i in prediction:\n",
    "            pre_ans = i.argmax()  # 예측 레이블\n",
    "            print(i)\n",
    "            print(pre_ans)\n",
    "            pre_ans_str = ''\n",
    "            if pre_ans == 0: pre_ans_str = \"o\"\n",
    "            elif pre_ans == 1: pre_ans_str = \"V\"\n",
    "            elif pre_ans == 2: pre_ans_str = \"paper\"\n",
    "            elif pre_ans == 3: pre_ans_str = \"rock\"\n",
    "            elif pre_ans == 4: pre_ans_str = \"side\"\n",
    "            \n",
    "            if i[0] >= 0.8 : \n",
    "                print(\"o\")\n",
    "                y=4\n",
    "            if i[1] >= 0.8: \n",
    "                print(\"v\")\n",
    "                y=5\n",
    "            if i[2] >= 0.8:\n",
    "                print(\"paper\") \n",
    "                y=1\n",
    "            if i[3] >= 0.8:\n",
    "                print(\"rock\") \n",
    "                y=2\n",
    "            if i[4] >= 0.8:\n",
    "                print(\"side\") \n",
    "                y=3\n",
    "            cnt += 1\n",
    "\n",
    "        if y == 1:\n",
    "            chgkey(1)\n",
    "        elif y == 2:\n",
    "            chgkey(2)\n",
    "        elif y == 3:\n",
    "            chgkey(3)\n",
    "        elif y == 4:\n",
    "            brightness[0] -= 10\n",
    "            if brightness[0] < 0 :\n",
    "                brightness[0] = 0\n",
    "            sbc.set_brightness(brightness[0])\n",
    "        elif y == 5:\n",
    "            brightness[0] += 10\n",
    "            if brightness[0] > 100:\n",
    "                brightness[0] = 100\n",
    "            sbc.set_brightness(brightness[0])\n",
    "        else : \n",
    "            print(\"인식되지 않음\")\n",
    "    elif 0xFF == ord('w'):\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b961d8f9206faaef7d3163d24c626b65937dc11e9ce5e74e2ec94a8fdbcc7de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
